{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Orange Juice Sales Forecasting**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Predict](#Predict)\n",
    "1. [Operationalize](#Operationalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example, we use AutoML to train, select, and operationalize a time-series forecasting model for multiple time-series.\n",
    "\n",
    "Make sure you have executed the [configuration notebook](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "The examples in the follow code samples use the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jp\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws = Workspace.from_config()\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\jp\\Documents\\GitHub\\vault-private')\n",
    "import credentials\n",
    "ws = credentials.authenticate_AZR('gmail','WS_demo','RG_wip')\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'test-automl-ojforecasting'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['SKU'] = ws.sku\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read this article on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-94c2e4e07cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Check if this compute target already exists in the workspace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mamlcompute_cluster_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcts\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mamlcompute_cluster_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'AmlCompute'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ws' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "# print('Checking cluster status...')\n",
    "# # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "# compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    \n",
    "# # For a more detailed view of current AmlCompute status, use get_status().\n",
    "\n",
    "# check compute targets\n",
    "cts = ws.compute_targets\n",
    "print(cts)\n",
    "\n",
    "# attach compute (gpu / cpu / local)\n",
    "import pyautogui\n",
    "sys.path.append(r'C:\\Users\\jp\\Documents\\GitHub\\jp-codes-python\\autoML_py36')\n",
    "import jp_utils\n",
    "answer = pyautogui.prompt(\n",
    "    text='Enter compute target (gpu, cpu, or local)',\n",
    "    title='Compute target',\n",
    "    default='cpu')\n",
    "compute_dict = {'gpu':'gpu-cluster', 'cpu':'cpu-cluster', 'local':'gpu-local'}\n",
    "target_name = jp_utils.generic_switch(compute_dict, answer)\n",
    "compute_target =cts[target_name]\n",
    "print(compute_target.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "You are now ready to load the historical orange juice sales data. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekStarting</th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>logQuantity</th>\n",
       "      <th>Advert</th>\n",
       "      <th>Price</th>\n",
       "      <th>Age60</th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>Hincome150</th>\n",
       "      <th>Large HH</th>\n",
       "      <th>Minorities</th>\n",
       "      <th>WorkingWoman</th>\n",
       "      <th>SSTRDIST</th>\n",
       "      <th>SSTRVOL</th>\n",
       "      <th>CPDIST5</th>\n",
       "      <th>CPWVOL5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>10560</td>\n",
       "      <td>9.26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>2</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>4480</td>\n",
       "      <td>8.41</td>\n",
       "      <td>0</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>2</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>8256</td>\n",
       "      <td>9.02</td>\n",
       "      <td>0</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>5</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>1792</td>\n",
       "      <td>7.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>10.92</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>5</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>4224</td>\n",
       "      <td>8.35</td>\n",
       "      <td>0</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>10.92</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WeekStarting  Store        Brand  Quantity  logQuantity  Advert  Price  \\\n",
       "0   1990-06-14      2    dominicks     10560         9.26       1   1.59   \n",
       "1   1990-06-14      2  minute.maid      4480         8.41       0   3.17   \n",
       "2   1990-06-14      2    tropicana      8256         9.02       0   3.87   \n",
       "3   1990-06-14      5    dominicks      1792         7.49       1   1.59   \n",
       "4   1990-06-14      5  minute.maid      4224         8.35       0   2.99   \n",
       "\n",
       "   Age60  COLLEGE  INCOME  Hincome150  Large HH  Minorities  WorkingWoman  \\\n",
       "0   0.23     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "1   0.23     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "2   0.23     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "3   0.12     0.32   10.92        0.54      0.10        0.05          0.41   \n",
       "4   0.12     0.32   10.92        0.54      0.10        0.05          0.41   \n",
       "\n",
       "   SSTRDIST  SSTRVOL  CPDIST5  CPWVOL5  \n",
       "0      2.11     1.14     1.93     0.38  \n",
       "1      2.11     1.14     1.93     0.38  \n",
       "2      2.11     1.14     1.93     0.38  \n",
       "3      3.80     0.68     1.60     0.74  \n",
       "4      3.80     0.68     1.60     0.74  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_column_name = 'WeekStarting'\n",
    "data = pd.read_csv(\"dominicks_OJ.csv\", parse_dates=[time_column_name])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the DataFrame holds a quantity of weekly sales for an OJ brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
    "\n",
    "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we thus define the **grain** - the columns whose values determine the boundaries between time-series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 249 individual time-series.\n"
     ]
    }
   ],
   "source": [
    "grain_column_names = ['Store', 'Brand']\n",
    "nseries = data.groupby(grain_column_names).ngroups\n",
    "print('Data contains {0} individual time-series.'.format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we extract sales time-series for just a few of the stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data subset contains 9 individual time-series.\n"
     ]
    }
   ],
   "source": [
    "use_stores = [2, 5, 8]\n",
    "data_subset = data[data.Store.isin(use_stores)]\n",
    "nseries = data_subset.groupby(grain_column_names).ngroups\n",
    "print('Data subset contains {0} individual time-series.'.format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "We now split the data into a training and a testing set for later forecast evaluation. The test set will contain the final 20 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the grain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_periods = 20\n",
    "\n",
    "def split_last_n_by_grain(df, n):\n",
    "    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
    "    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
    "                  .groupby(grain_column_names, group_keys=False))\n",
    "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
    "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
    "    return df_head, df_tail\n",
    "\n",
    "train, test = split_last_n_by_grain(data_subset, n_test_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to datastore\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the train and test data and create [tabular datasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training and testing. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv (r'./dominicks_OJ_train.csv', index = None, header=True)\n",
    "test.to_csv (r'./dominicks_OJ_test.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./dominicks_OJ_train.csv', './dominicks_OJ_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/dominicks_OJ_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_pandas_dataframe().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For forecasting tasks, AutoML uses pre-processing and estimation steps that are specific to time-series. AutoML will undertake the following pre-processing steps:\n",
    "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
    "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
    "* Create grain-based features to enable fixed effects across different series\n",
    "* Create time-based features to assist in learning seasonal patterns\n",
    "* Encode categorical variables to numeric quantities\n",
    "\n",
    "In this notebook, AutoML will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series. If you're looking for training multiple models for different time-series, please check out the forecasting grouping notebook. \n",
    "\n",
    "You are almost ready to start an AutoML training job. First, we need to separate the target column from the rest of the DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'Quantity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The AutoMLConfig object defines the settings and data for an AutoML training job. Here, we set necessary inputs like the task type, the number of AutoML iterations to try, the training data, and cross-validation parameters. \n",
    "\n",
    "For forecasting tasks, there are some additional parameters that can be set: the name of the column holding the date/time, the grain column names, and the maximum forecast horizon. A time column is required for forecasting, while the grain is optional. If a grain is not given, AutoML assumes that the whole dataset is a single time-series. We also pass a list of columns to drop prior to modeling. The _logQuantity_ column is completely correlated with the target quantity, so it must be removed to prevent a target leak.\n",
    "\n",
    "The forecast horizon is given in units of the time-series frequency; for instance, the OJ series frequency is weekly, so a horizon of 20 means that a trained model will estimate sales up to 20 weeks beyond the latest date in the training data for each series. In this example, we set the maximum horizon to the number of samples per series in the test set (n_test_periods). Generally, the value of this parameter will be dictated by business needs. For example, a demand planning organizaion that needs to estimate the next month of sales would set the horizon accordingly. Please see the [energy_demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) for more discussion of forecast horizon.\n",
    "\n",
    "Finally, a note about the cross-validation (CV) procedure for time-series data. AutoML uses out-of-sample error estimates to select a best pipeline/model, so it is important that the CV fold splitting is done correctly. Time-series can violate the basic statistical assumptions of the canonical K-Fold CV strategy, so AutoML implements a [rolling origin validation](https://robjhyndman.com/hyndsight/tscv/) procedure to create CV folds for time-series data. To use this procedure, you just need to specify the desired number of CV folds in the AutoMLConfig object. It is also possible to bypass CV and use your own validation set by setting the *validation_data* parameter of AutoMLConfig.\n",
    "\n",
    "Here is a summary of AutoMLConfig parameters used for training the OJ model:\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**experiment_timeout_hours**|Experimentation timeout in hours.|\n",
    "|**enable_early_stopping**|If early stopping is on, training will stop when the primary metric is no longer improving.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection|\n",
    "|**enable_voting_ensemble**|Allow AutoML to create a Voting ensemble of the best performing models|\n",
    "|**enable_stack_ensemble**|Allow AutoML to create a Stack ensemble of the best performing models|\n",
    "|**debug_log**|Log file path for writing debugging information|\n",
    "|**time_column_name**|Name of the datetime column in the input data|\n",
    "|**grain_column_names**|Name(s) of the columns defining individual series in the input data|\n",
    "|**drop_column_names**|Name(s) of columns to drop prior to modeling|\n",
    "|**max_horizon**|Maximum desired forecast horizon in units of time-series frequency|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_column_name,\n",
    "    'grain_column_names': grain_column_names,\n",
    "    'drop_column_names': ['logQuantity'],  # 'logQuantity' is a leaky feature, so we remove it.\n",
    "    'max_horizon': n_test_periods\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl_oj_sales_errors.log',\n",
    "                             primary_metric='normalized_mean_absolute_error',\n",
    "                             experiment_timeout_hours=0.25,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping=True,\n",
    "                             n_cross_validations=3,\n",
    "                             verbosity=logging.INFO,\n",
    "                             **time_series_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now submit a new training run. Depending on the data and number of iterations this operation may take several minutes.\n",
    "Information from each iteration will be printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)\n",
    "remote_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Each run within an Experiment stores serialized (i.e. pickled) pipelines from the AutoML iterations. We can now retrieve the pipeline with the best performance on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "model_name = best_run.properties['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name)\n",
    "print(best_run.properties)\n",
    "print(fitted_model)\n",
    "\n",
    "from helper import get_result_df\n",
    "summary_df = get_result_df(remote_run)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test\n",
    "y_test = X_test.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekStarting</th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "      <th>logQuantity</th>\n",
       "      <th>Advert</th>\n",
       "      <th>Price</th>\n",
       "      <th>Age60</th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>Hincome150</th>\n",
       "      <th>Large HH</th>\n",
       "      <th>Minorities</th>\n",
       "      <th>WorkingWoman</th>\n",
       "      <th>SSTRDIST</th>\n",
       "      <th>SSTRVOL</th>\n",
       "      <th>CPDIST5</th>\n",
       "      <th>CPWVOL5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24192</th>\n",
       "      <td>1992-05-21</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>9.18</td>\n",
       "      <td>0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>1992-05-28</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24675</th>\n",
       "      <td>1992-06-04</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24909</th>\n",
       "      <td>1992-06-11</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25152</th>\n",
       "      <td>1992-06-18</td>\n",
       "      <td>2</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>8.52</td>\n",
       "      <td>0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekStarting  Store      Brand  logQuantity  Advert  Price  Age60  \\\n",
       "24192   1992-05-21      2  dominicks         9.18       0   1.69   0.23   \n",
       "24441   1992-05-28      2  dominicks        10.73       0   1.69   0.23   \n",
       "24675   1992-06-04      2  dominicks         9.95       0   1.74   0.23   \n",
       "24909   1992-06-11      2  dominicks         8.79       0   2.09   0.23   \n",
       "25152   1992-06-18      2  dominicks         8.52       0   2.05   0.23   \n",
       "\n",
       "       COLLEGE  INCOME  Hincome150  Large HH  Minorities  WorkingWoman  \\\n",
       "24192     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "24441     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "24675     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "24909     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "25152     0.25   10.55        0.46      0.10        0.11          0.30   \n",
       "\n",
       "       SSTRDIST  SSTRVOL  CPDIST5  CPWVOL5  \n",
       "24192      2.11     1.14     1.93     0.38  \n",
       "24441      2.11     1.14     1.93     0.38  \n",
       "24675      2.11     1.14     1.93     0.38  \n",
       "24909      2.11     1.14     1.93     0.38  \n",
       "25152      2.11     1.14     1.93     0.38  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The featurized data, aligned to y, will also be returned.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "# and helps align the forecast to the original data\n",
    "y_predictions, X_trans = fitted_model.forecast(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are used to scikit pipelines, perhaps you expected `predict(X_test)`. However, forecasting requires a more general interface that also supplies the past target `y` values. Please use `forecast(X,y)` as `predict(X)` is reserved for internal purposes on forecasting models.\n",
    "\n",
    "The [energy demand forecasting notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) demonstrates the use of the forecast function in more detail in the context of using lags and rolling window features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE). \n",
    "\n",
    "It is a good practice to always align the output explicitly to the input, as the count and order of the rows may have changed during transformations that span multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_helper import align_outputs\n",
    "\n",
    "df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core._vendor.automl.client.core.common import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from automl.client.core.common import constants\n",
    "\n",
    "# use automl metrics module\n",
    "scores = metrics.compute_metrics_regression(\n",
    "    df_all['predicted'],\n",
    "    df_all[target_column_name],\n",
    "    list(constants.Metric.SCALAR_REGRESSION_SET),\n",
    "    None, None, None)\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Operationalization_ means getting the model into the cloud so that other can run it after you close the notebook. We will create a docker running on Azure Container Instances with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'AutoML OJ forecaster'\n",
    "tags = None\n",
    "model = remote_run.register_model(model_name = model_name, description = description, tags = tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remote_run.model_id)\n",
    "print([model.name, ' ', model.run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the scoring script\n",
    "\n",
    "For the deployment we need a function which will run the forecast on serialized data. It can be obtained from the best_run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_name = 'score_fcast.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', script_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model as a Web Service on Azure Container Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "inference_config = InferenceConfig(environment = best_run.get_environment(), \n",
    "                                   entry_script = script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               tags = {'type': \"automl-forecasting\"},\n",
    "                                               description = \"Automl forecasting sample service\")\n",
    "\n",
    "aci_service_name = 'automl-oj-forecast-01'\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "X_query = X_test.copy()\n",
    "# We have to convert datetime to string, because Timestamps cannot be serialized to JSON.\n",
    "X_query[time_column_name] = X_query[time_column_name].astype(str)\n",
    "# The Service object accept the complex dictionary, which is internally converted to JSON string.\n",
    "# The section 'data' contains the data frame in the form of dictionary.\n",
    "test_sample = json.dumps({'data': X_query.to_dict(orient='records')})\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aci_service.run(input_data = test_sample)\n",
    "# translate from networkese to datascientese\n",
    "try: \n",
    "    res_dict = json.loads(response)\n",
    "    y_fcst_all = pd.DataFrame(res_dict['index'])\n",
    "    y_fcst_all[time_column_name] = pd.to_datetime(y_fcst_all[time_column_name], unit = 'ms')\n",
    "    y_fcst_all['forecast'] = res_dict['forecast']    \n",
    "except:\n",
    "    print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fcst_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the web service if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv = Webservice(ws, 'automl-oj-forecast-01')\n",
    "serv.delete()     # don't do it accidentally"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "erwright"
   }
  ],
  "category": "tutorial",
  "celltoolbar": "Raw Cell Format",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Orange Juice Sales"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML AutoML"
  ],
  "friendly_name": "Forecasting orange juice sales with deployment",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py36_cloudML': conda)",
   "language": "python",
   "name": "python361064bitpy36cloudmlcondac22f711eaaa348dbbcd5b69f0cdd3d71"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "tags": [
   "None"
  ],
  "task": "Forecasting",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
